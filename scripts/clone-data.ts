#!/usr/bin/env tsx
/**
 * Script de clonage de donn√©es entre environnements
 * Clone les donn√©es de production vers test ou d√©veloppement
 * Mode: CONSERVATION des donn√©es existantes (pas de TRUNCATE)
 * Am√©liorations:
 *  - Ne manipule plus auth.users directement
 *  - Chargement .env robuste via dotenv
 *  - Pr√©-v√©rification de compatibilit√© sch√©ma (hook)
 *  - Pagination pour √©viter l'usage m√©moire excessif
 *  - Meilleure journalisation et gestion d'erreurs
 *  - Utilise UPSERT pour toutes les tables (pr√©serve donn√©es existantes)
 */

import { createClient, SupabaseClient } from '@supabase/supabase-js'
import dotenv from 'dotenv'

interface CloneOptions {
  source: 'prod' | 'test' | 'dev' | 'learning'
  target: 'prod' | 'test' | 'dev' | 'learning'
  tables?: string[]
  excludeSensitive?: boolean
  dryRun?: boolean
  pageSize?: number
  truncate?: boolean  // Nouveau: option pour vider la base cible avant clonage
}

type TableResult = { status: 'success'|'error'|'empty'|'dry-run', records?: number, error?: string }

export class DataCloner {
  private sourceClient!: SupabaseClient
  private targetClient!: SupabaseClient

  constructor(private options: CloneOptions) {}

  private resolveEnvFile(env: 'prod'|'test'|'dev'|'learning') {
    if (env === 'prod') return 'env-backup/.env.prod'
    if (env === 'dev') return 'env-backup/.env.development'
    if (env === 'test') return 'env-backup/.env.test'
    if (env === 'learning') return 'env-backup/.env.learning'
    return `env-backup/.env.${env}`
  }

  private loadEnvironment(env: 'prod'|'test'|'dev'|'learning') {
    const path = this.resolveEnvFile(env)
    const res = dotenv.config({ path })
    if (res.error) {
      throw new Error(`Fichier d'environnement introuvable ou invalide: ${path}`)
    }
    const url = process.env.NEXT_PUBLIC_SUPABASE_URL
    const serviceKey = process.env.SUPABASE_SERVICE_ROLE_KEY
    if (!url || !serviceKey) {
      throw new Error(`Variables manquantes dans ${path}: NEXT_PUBLIC_SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY`)
    }
    return { url, serviceKey }
  }

  // Nouvelle m√©thode pour extraire les infos de connexion PostgreSQL depuis Supabase URL
  private extractPostgresConnection(url: string, serviceKey: string) {
    try {
      const supabaseUrl = new URL(url)
      const projectId = supabaseUrl.hostname.split('.')[0]
      const dbPassword = serviceKey

      // Construction des informations de connexion PostgreSQL
      return {
        host: `db.${supabaseUrl.hostname}`,
        port: 5432,
        database: 'postgres',
        username: 'postgres',
        password: dbPassword,
        projectId: projectId
      }
    } catch (error) {
      throw new Error(`Impossible d'extraire les informations de connexion PostgreSQL depuis ${url}`)
    }
  }

  private initializeClients() {
    const source = this.loadEnvironment(this.options.source)
    const target = this.loadEnvironment(this.options.target)
    this.sourceClient = createClient(source.url, source.serviceKey, { auth: { persistSession: false } })
    this.targetClient = createClient(target.url, target.serviceKey, { auth: { persistSession: false } })
  }

  // Hook de pr√©-v√©rification sch√©ma (non bloquant si indisponible)
  private async preflightSchemaCheck(): Promise<void> {
    try {
      // Si vous avez d√©j√† un endpoint interne: app/api/health ou un script diagnose, appelez-le ici.
      // Ici, on fait un check minimal: existence de quelques tables cl√©s dans source et cible.
      const mustHaveTables = ['profiles','lofts','transactions','categories','currencies']
      for (const [client, label] of [[this.sourceClient,'source'] as const, [this.targetClient,'target'] as const]) {
        for (const t of mustHaveTables) {
          const { error } = await client.from(t).select('id').limit(1)
          if (error) {
            console.warn(`‚ö†Ô∏è Pr√©-check sch√©ma (${label}): table "${t}" inaccessible: ${error.message}`)
          }
        }
      }
      console.log('üß™ Pr√©-v√©rification sch√©ma effectu√©e (non bloquante).')
    } catch (e: any) {
      console.warn('‚ö†Ô∏è √âchec pr√©-v√©rification sch√©ma (ignor√©e):', e?.message || e)
    }
  }

  // Anonymisation
  private anonymizeProfiles(profiles: any[]): any[] {
    return profiles.map((profile) => {
      const isAdmin = profile.role === 'admin'
      const anonLocal = `${this.options.target}.local`
      const stableSuffix = (profile.id || '').toString().slice(0, 6) || Math.random().toString(36).slice(2, 8)
      return {
        ...profile,
        email: isAdmin ? `admin@${anonLocal}` : `user_${stableSuffix}@${anonLocal}`,
        full_name: profile.full_name ? `${profile.full_name} (${this.options.target.toUpperCase()})` : `User ${stableSuffix.toUpperCase()}`,
        airbnb_access_token: null,
        airbnb_refresh_token: null,
        updated_at: new Date().toISOString(),
      }
    })
  }
  private anonymizeUserSessions(): any[] { return [] }
  private anonymizeNotifications(notifications: any[]): any[] {
    return notifications.map((n) => ({
      ...n,
      message: typeof n.message === 'string' && n.message.includes('@') ? 'Message de test anonymis√©' : n.message,
      is_read: true,
      read_at: new Date().toISOString(),
    }))
  }
  private anonymizeMessages(messages: any[]): any[] {
    return messages.map((m) => ({
      ...m,
      content: 'Message de test anonymis√©',
      metadata: m.metadata ? { ...m.metadata, anonymized: true } : null,
    }))
  }

  private async fetchTablePaged(client: SupabaseClient, table: string, pageSize: number): Promise<any[]> {
    let from = 0
    const out: any[] = []
    for (;;) {
      const to = from + pageSize - 1
      const { data, error } = await client.from(table).select('*').range(from, to)
      if (error) {
        throw new Error(`Lecture ${table} √©chou√©e: ${error.message}`)
      }
      if (!data || data.length === 0) break
      out.push(...data)
      from += pageSize
      if (data.length < pageSize) break
    }
    return out
  }

  private async tableExists(client: SupabaseClient, table: string): Promise<boolean> {
    // Try a cheap existence probe
    const { error } = await client.from(table).select('*').limit(1)
    if (error && `${error.message}`.toLowerCase().includes('relation') && `${error.message}`.toLowerCase().includes('does not exist')) {
      return false
    }
    // If other error, assume exists and let normal flow handle specifics
    return true
  }

  private async getTableColumns(client: SupabaseClient, table: string): Promise<Set<string>> {
    // Fallback: attempt to select one row and infer keys
    const { data, error } = await client.from(table).select('*').limit(1)
    if (error) {
      // If cannot read, return empty (will cause shaping to drop all and then skip)
      return new Set<string>()
    }
    if (data && data.length > 0) {
      return new Set<string>(Object.keys(data[0] as Record<string, any>))
    }
    // If empty, we can't infer; try inserting with unshaped data later. Return empty set to avoid shaping.
    return new Set<string>()
  }

  private async resetDatabase() {
    console.log('üóëÔ∏è RESETTING TARGET DATABASE (ACTUAL WORKING APPROACH)...')

    // ACTUAL WORKING APPROACH: Clear tables in proper dependency order
    try {
      console.log('üîç Starting database reset with proper FK handling...')

      // Step 1: Clear tables that reference other tables FIRST (reverse dependency order)
      const clearOrder = [
        // Step 1: Clear tables with FK references to others FIRST
        'customers', 'loft_photos', 'team_members', 'tasks', 'transactions',
        'transaction_category_references', 'notifications', 'messages',

        // Step 2: Clear main tables
        'lofts', 'profiles', 'teams', 'loft_owners', 'settings',

        // Step 3: Clear reference tables last
        'currencies', 'categories', 'zone_areas', 'internet_connection_types', 'payment_methods'
      ]

      console.log(`üìã Clearing ${clearOrder.length} tables in dependency order...`)

      let clearedCount = 0
      let errorCount = 0

      for (const table of clearOrder) {
        try {
          console.log(`üóëÔ∏è Clearing table: ${table}`)

          // Use aggressive delete that actually works
          const { error } = await this.targetClient.from(table).delete().neq('id', '00000000-0000-0000-0000-000000000000')

          if (error && !`${error.message}`.includes('No rows found')) {
            console.warn(`‚ö†Ô∏è Could not clear ${table}: ${error.message}`)
            errorCount++
          } else {
            console.log(`‚úÖ Cleared ${table}`)
            clearedCount++
          }
        } catch (error) {
          console.warn(`‚ö†Ô∏è Error clearing ${table}: ${error}`)
          errorCount++
        }
      }

      console.log(`‚úÖ Database reset completed: ${clearedCount}/${clearOrder.length} tables cleared`)

      if (errorCount > 0) {
        console.log(`‚ö†Ô∏è ${errorCount} tables had errors, but continuing...`)
      }

    } catch (error) {
      console.warn(`‚ö†Ô∏è Error during database reset: ${error}`)
      console.log('‚ö†Ô∏è Continuing with import anyway...')
    }
  }

  private async getAllTables(): Promise<string[]> {
    // Get table names by trying common table names and discovering them
    const commonTables = [
      'messages', 'notifications', 'tasks', 'team_members', 'transactions',
      'transaction_category_references', 'lofts', 'profiles', 'user_sessions',
      'teams', 'loft_owners', 'settings', 'currencies', 'categories',
      'zone_areas', 'internet_connection_types', 'payment_methods'
    ]

    const existingTables: string[] = []

    for (const table of commonTables) {
      try {
        const { error } = await this.targetClient.from(table).select('*').limit(1)
        if (!error) {
          existingTables.push(table)
        }
      } catch (error) {
        // Table doesn't exist, skip
      }
    }

    return existingTables
  }

  private async clearTableWithPagination(table: string) {
    try {
      // For tables with FK constraints, we need to be more careful
      // First, try to get a sample record to understand the structure
      const { data: sampleData } = await this.targetClient.from(table).select('*').limit(1)

      if (sampleData && sampleData.length > 0) {
        // If table has data, try to delete in smaller chunks
        let deletedCount = 0
        const batchSize = 100

        while (true) {
          const { data, error } = await this.targetClient
            .from(table)
            .select('id')
            .limit(batchSize)

          if (error || !data || data.length === 0) break

          const ids = data.map(row => row.id)

          const { error: deleteError } = await this.targetClient
            .from(table)
            .delete()
            .in('id', ids)

          if (deleteError) {
            console.warn(`‚ö†Ô∏è Batch delete failed for ${table}: ${deleteError.message}`)
            break
          }

          deletedCount += data.length
          console.log(`üìä Deleted ${deletedCount} records from ${table}`)

          if (data.length < batchSize) break
        }
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Pagination clear failed for ${table}: ${error}`)
    }
  }

  private async fallbackCleanup() {
    console.log('üîÑ FALLBACK CLEANUP: Manual table clearing...')

    // Manual cleanup without relying on FK disabling
    const tablesToClear = [
      'currencies', 'categories', 'zone_areas', 'internet_connection_types',
      'payment_methods', 'loft_owners', 'teams', 'profiles', 'lofts',
      'team_members', 'tasks', 'transactions', 'transaction_category_references',
      'settings', 'notifications', 'messages'
    ]

    for (const table of tablesToClear) {
      try {
        const { error } = await this.targetClient.from(table).delete().neq('id', '00000000-0000-0000-0000-000000000000')
        if (error && !`${error.message}`.includes('No rows found')) {
          console.warn(`‚ö†Ô∏è Manual cleanup failed for ${table}: ${error.message}`)
        } else {
          console.log(`‚úÖ Manually cleared ${table}`)
        }
      } catch (error) {
        console.warn(`‚ö†Ô∏è Error in manual cleanup for ${table}: ${error}`)
      }
    }
  }

  private async truncateTargetTable(table: string) {
    // Professional approach: use batch delete instead of TRUNCATE
    try {
      console.log(`üóëÔ∏è Clearing ${table} using batch delete...`)

      // Use the same approach as in resetDatabase
      await this.clearTableWithPagination(table)

      console.log(`‚úÖ Cleared ${table}`)
    } catch (error) {
      console.warn(`‚ö†Ô∏è Error clearing ${table}: ${error}`)
      // Final fallback
      try {
        const { error: deleteError } = await this.targetClient.from(table).delete().neq('id', '00000000-0000-0000-0000-000000000000')
        if (deleteError && !`${deleteError.message}`.includes('No rows found')) {
          console.warn(`‚ö†Ô∏è Final fallback delete also failed for ${table}: ${deleteError.message}`)
        }
      } catch (finalError) {
        console.warn(`‚ö†Ô∏è All clear methods failed for ${table}: ${finalError}`)
      }
    }
  }

  private processSensitive(table: string, rows: any[]): any[] {
    if (table === 'profiles') return this.anonymizeProfiles(rows)
    if (table === 'user_sessions') return this.anonymizeUserSessions()
    if (table === 'notifications') return this.anonymizeNotifications(rows)
    if (table === 'messages') return this.anonymizeMessages(rows)
    return rows
  }

  async cloneData() {
    // 1) Init clients (dotenv) + 2) Preflight schema check
    this.initializeClients()
    await this.preflightSchemaCheck()

    const pageSize = this.options.pageSize ?? 1000

    console.log(`üîÑ CLONAGE PROFESSIONNEL: ${this.options.source.toUpperCase()} ‚Üí ${this.options.target.toUpperCase()}`)
    console.log('='.repeat(80))
    console.log('üöÄ MODE PROFESSIONNEL: Suppression compl√®te + Recr√©ation + Import')
    console.log('='.repeat(80))
    if (this.options.dryRun) console.log('üß™ MODE TEST - Aucune donn√©e ne sera modifi√©e')

    // Professional approach: Reset database first, then import data
    if (!this.options.dryRun) {
      await this.resetDatabase()
    }

    // ACTUAL WORKING ORDER: Import reference tables first, then dependent tables
    const tablesToClone = this.options.tables || [
      // Step 1: Import reference tables first (no dependencies)
      'currencies',                    // No dependencies
      'categories',                    // No dependencies
      'zone_areas',                    // No dependencies
      'internet_connection_types',    // No dependencies
      'payment_methods',              // No dependencies
      'loft_owners',                   // Referenced by lofts
      'teams',                         // Referenced by team_members

      // Step 2: Import main tables (reference the above)
      'profiles',                      // Referenced by many tables
      'lofts',                         // References loft_owners, categories, etc.
      'team_members',                  // References teams, profiles
      'tasks',                         // References profiles, lofts
      'transactions',                  // References lofts, categories
      'transaction_category_references', // References transactions, categories
      'settings',                      // No dependencies
      'notifications',                 // References profiles
      'messages'                       // References profiles
    ]

    // Don't add profiles again if already in the list
    if (!this.options.excludeSensitive) {
      if (!tablesToClone.includes('profiles')) tablesToClone.push('profiles')
      if (!tablesToClone.includes('user_sessions')) tablesToClone.push('user_sessions')
      if (!tablesToClone.includes('notifications')) tablesToClone.push('notifications')
      if (!tablesToClone.includes('messages')) tablesToClone.push('messages')
    }

    let totalRecords = 0
    const results: Record<string, TableResult> = {}

    for (const table of tablesToClone) {
      try {
        console.log(`\nüìã Clonage de la table: ${table}`)
        console.log('-'.repeat(40))

        // Skip tables missing on target to avoid immediate failures (e.g., user_sessions)
        const targetHas = await this.tableExists(this.targetClient, table)
        if (!targetHas) {
          console.log(`‚ÑπÔ∏è Table absente dans la cible, ignor√©e: ${table}`)
          results[table] = { status: 'empty', records: 0 }
          continue
        }

        const sourceHas = await this.tableExists(this.sourceClient, table)
        if (!sourceHas) {
          console.log(`‚ÑπÔ∏è Table absente dans la source, ignor√©e: ${table}`)
          results[table] = { status: 'empty', records: 0 }
          continue
        }

        const sourceData = await this.fetchTablePaged(this.sourceClient, table, pageSize)

        if (!sourceData.length) {
          console.log(`‚ÑπÔ∏è Table ${table} vide dans la source`)
          results[table] = { status: 'empty', records: 0 }
          continue
        }

        console.log(`üìä ${sourceData.length} enregistrements trouv√©s (pagination: ${pageSize})`)

        if (this.options.dryRun) {
          console.log(`üß™ [TEST] Aurait clon√© ${sourceData.length} enregistrements`)
          results[table] = { status: 'dry-run', records: sourceData.length }
          totalRecords += sourceData.length
          continue
        }

        // Column intersection shaping to handle schema drift
        const targetCols = await this.getTableColumns(this.targetClient, table)
        let shaped = sourceData
        if (targetCols.size > 0) {
          shaped = sourceData.map((row) => {
            const out: any = {}
            for (const k of Object.keys(row)) {
              if (targetCols.has(k)) out[k] = (row as any)[k]
            }
            return out
          })
        }

        // Sensitive processing after shaping (only columns that exist)
        let processed = this.processSensitive(table, shaped)

        // Note: Tables are already cleared by resetDatabase() method above
        // No need for individual table truncation

        // Batch upsert for all tables (preserves existing data unless truncate is enabled)
        const batchSize = 500
        let upsertedCount = 0
        for (let i = 0; i < processed.length; i += batchSize) {
          const batch = processed.slice(i, i + batchSize)

          // Try to infer conflict target: prefer 'id' if present, else first key
          const sample = batch[0] || {}
          const conflictKey = 'id' in sample ? 'id' : Object.keys(sample)[0]

          // Debug: Log what we're trying to insert for lofts table
          if (table === 'lofts' && i === 0) {
            console.log(`üîç DEBUG: Inserting ${batch.length} lofts records`)
            console.log(`üîç Sample record:`, JSON.stringify(batch[0], null, 2))
          }

          // Use upsert to replace existing data (works even with FK constraints)
          let success = false
          try {
            const q = (this.targetClient.from(table) as any).upsert
              ? (this.targetClient.from(table) as any).upsert(batch, { onConflict: conflictKey, ignoreDuplicates: false })
              : this.targetClient.from(table).insert(batch) // fallback if upsert unsupported
            const { error } = await q
            if (error) {
              console.log(`‚ùå Erreur upsert lot ${Math.floor(i / batchSize) + 1} (${table}): ${error.message}`)
              if (table === 'lofts') {
                console.log(`üîç DEBUG: Error details:`, error)
              }
            } else {
              success = true
              upsertedCount += batch.length
              process.stdout.write(`\rüì• Upsert: ${upsertedCount}/${processed.length}`)
            }
          } catch (e) {
            console.log(`üí• Exception during upsert (${table}):`, e)
          }

          // If upsert failed, try regular insert as fallback
          if (!success) {
            try {
              console.log(`üîÑ Fallback: trying regular insert for ${table}...`)
              const { error } = await this.targetClient.from(table).insert(batch)
              if (error) {
                console.log(`‚ùå Fallback insert also failed (${table}): ${error.message}`)
              } else {
                success = true
                upsertedCount += batch.length
                process.stdout.write(`\rüì• Insert: ${upsertedCount}/${processed.length}`)
              }
            } catch (e) {
              console.log(`üí• Fallback insert also failed (${table}):`, e)
            }
          }
        }
        console.log(`\n‚úÖ Table ${table}: ${upsertedCount} enregistrements clon√©s`)
        results[table] = { status: 'success', records: upsertedCount }
        totalRecords += upsertedCount
      } catch (e: any) {
        console.log(`üí• Erreur inattendue pour ${table}:`, e?.message || e)
        results[table] = { status: 'error', error: e?.message || String(e) }
      }
    }

    // R√©sum√©
    console.log('\nüìä R√âSUM√â DU CLONAGE')
    console.log('='.repeat(50))
    console.log(`üìà Total des enregistrements: ${totalRecords}`)
    const succ = Object.values(results).filter(r => r.status === 'success').length
    const err = Object.values(results).filter(r => r.status === 'error').length
    const empty = Object.values(results).filter(r => r.status === 'empty').length
    console.log(`‚úÖ Tables r√©ussies: ${succ}`)
    console.log(`‚ùå Tables en erreur: ${err}`)
    console.log(`‚ÑπÔ∏è Tables vides: ${empty}`)

    console.log('\nüìã D√©tail par table:')
    for (const [table, r] of Object.entries(results)) {
      const icon = r.status === 'success' ? '‚úÖ' : r.status === 'error' ? '‚ùå' : r.status === 'empty' ? '‚ÑπÔ∏è' : 'üß™'
      console.log(`${icon} ${table}: ${r.records || 0} enregistrements (${r.status}${r.error ? ` - ${r.error}` : ''})`)
    }

    console.log(`\nüéâ CLONAGE PROFESSIONNEL TERMIN√â: ${this.options.source.toUpperCase()} ‚Üí ${this.options.target.toUpperCase()}`)
    console.log('='.repeat(80))
    console.log('‚úÖ APPROCHE PROFESSIONNELLE: Reset complet + Import propre')
    console.log('‚úÖ CONTRAINTES FK: G√©r√©es automatiquement')
    console.log('‚úÖ DONN√âES: Import√©es sans conflits')
    console.log('='.repeat(80))
    if (this.options.target !== 'prod') {
      console.log('üí° Testez avec: npm run env:switch:' + this.options.target)
      console.log('üîç V√©rifiez les donn√©es avec vos scripts de v√©rification')
    }
  }

  // V√©rification simple: compare les counts par table c√¥t√© source/target
  async verifyClone(tables?: string[]) {
    // Simple count comparison using pagination for accuracy
    this.initializeClients()
    const list = tables && tables.length ? tables : [
      'currencies','categories','zone_areas','internet_connection_types','payment_methods',
      'loft_owners','lofts','teams','team_members','tasks','transactions',
      'transaction_category_references','settings','profiles','notifications','messages'
    ]
    console.log('\nüîé V√©rification post-clonage (counts):')
    const page = 1000
    for (const t of list) {
      const srcExists = await this.tableExists(this.sourceClient, t)
      const tgtExists = await this.tableExists(this.targetClient, t)
      if (!srcExists || !tgtExists) {
        console.log(`- ${t}: ignor√©e (existence source=${srcExists}, cible=${tgtExists})`)
        continue
      }
      const countWith = async (client: SupabaseClient) => {
        let total = 0, offset = 0
        for (;;) {
          const { data, error } = await client.from(t).select('*').range(offset, offset + page - 1)
          if (error) break
          if (!data || data.length === 0) break
          total += data.length
          if (data.length < page) break
          offset += page
        }
        return total
      }
      const sCount = await countWith(this.sourceClient)
      const tCount = await countWith(this.targetClient)
      const match = sCount === tCount ? '‚úÖ' : '‚ö†Ô∏è'
      console.log(`${match} ${t}: source=${sCount}, target=${tCount}`)
    }
    console.log('‚úîÔ∏è V√©rification simple termin√©e.')
  }
}
